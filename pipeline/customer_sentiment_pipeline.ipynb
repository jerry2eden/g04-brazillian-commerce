{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the necessary pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-20.3.1-py2.py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.4\n",
      "    Uninstalling pip-20.2.4:\n",
      "      Successfully uninstalled pip-20.2.4\n",
      "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.6 are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed pip-20.3.1\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: pandas==0.23.4 in /home/jovyan/.local/lib/python3.6/site-packages (0.23.4)\n",
      "Requirement already satisfied: matplotlib==3.0.3 in /home/jovyan/.local/lib/python3.6/site-packages (3.0.3)\n",
      "Requirement already satisfied: scipy==1.2.1 in /home/jovyan/.local/lib/python3.6/site-packages (1.2.1)\n",
      "Requirement already satisfied: scikit-learn==0.22 in /home/jovyan/.local/lib/python3.6/site-packages (0.22)\n",
      "Requirement already satisfied: tensorflow==2.0 in /home/jovyan/.local/lib/python3.6/site-packages (2.0.0)\n",
      "Requirement already satisfied: keras==1.2.2 in /home/jovyan/.local/lib/python3.6/site-packages (1.2.2)\n",
      "Requirement already satisfied: theano in /home/jovyan/.local/lib/python3.6/site-packages (from keras==1.2.2) (1.0.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.13.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.18.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.23.4) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.18.1)\n",
      "Requirement already satisfied: scipy==1.2.1 in /home/jovyan/.local/lib/python3.6/site-packages (1.2.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/jovyan/.local/lib/python3.6/site-packages (from scikit-learn==0.22) (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.18.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.26.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.0.8)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.11.2)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow==2.0) (2.0.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow==2.0) (2.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.13.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.18.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==2.0) (0.30.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.2.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.1.8)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.11.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.13.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.13.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.13.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.13.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.18.1)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.18.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.18.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.13.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.0.3) (44.0.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.18.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.0.3) (44.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.13.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.18.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.26.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.22.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.11.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.13.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==2.0) (0.30.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.9.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.10.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.16.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.0.3) (44.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.2.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.0.3) (44.0.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.13.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.3.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.0.3) (44.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.25.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.22.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.8)\n",
      "Requirement already satisfied: scipy==1.2.1 in /home/jovyan/.local/lib/python3.6/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3) (1.18.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2.2) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "\n",
    "!pip3 install pandas==0.23.4 matplotlib==3.0.3 scipy==1.2.1 scikit-learn==0.22 tensorflow==2.0 keras==1.2.2 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installling kubeflow pipeline SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: kfp in /home/jovyan/.local/lib/python3.6/site-packages (1.1.1)\n",
      "Collecting kfp\n",
      "  Downloading kfp-1.1.2.tar.gz (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.25.0)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (10.0.1)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.10.0)\n",
      "Requirement already satisfied: requests_toolbelt>=0.8.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: tabulate in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied: click in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: Deprecated in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (1.2.10)\n",
      "Requirement already satisfied: strip-hints in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: docstring-parser>=0.7.3 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.7.3)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kfp) (0.1.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (44.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.0)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.10.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (44.0.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.11.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.52.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2019.3)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.10.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.11.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (1.4.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (44.0.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.7)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (0.6.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.1b1\n",
      "  Downloading kfp-server-api-1.1.2rc1.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (1.25.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2.8.1)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.10.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (1.25.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (44.0.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2.8.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2019.11.28)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (44.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.6)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (2019.11.28)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=1.1.1b1->kfp) (1.25.7)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (1.13.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata->jsonschema>=3.0.1->kfp) (8.0.2)\n",
      "Building wheels for collected packages: kfp, kfp-server-api\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.1.2-py3-none-any.whl size=219914 sha256=515773ce7b039227b65007fd45bb420565d78d50eaad460304bcff5aa6577581\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/4e/71/3f/79a5ffcfb1346020bd33519235f0f39a8ff6e3fc052b77552d\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp-server-api: filename=kfp_server_api-1.1.2rc1-py3-none-any.whl size=108990 sha256=4e4255455bafccea7f55755ed0abfb70c7dbbb0fa46b70e5917c2c614efe34c4\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/3f/59/a4/9864fcf13058ac28e76ba8aafc044df66b2bc7559e12d8155f\n",
      "Successfully built kfp kfp-server-api\n",
      "Installing collected packages: kfp-server-api, kfp\n",
      "  Attempting uninstall: kfp-server-api\n",
      "    Found existing installation: kfp-server-api 1.0.1\n",
      "    Uninstalling kfp-server-api-1.0.1:\n",
      "      Successfully uninstalled kfp-server-api-1.0.1\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 1.1.1\n",
      "    Uninstalling kfp-1.1.1:\n",
      "      Successfully uninstalled kfp-1.1.1\n",
      "\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed kfp-1.1.2 kfp-server-api-1.1.2rc1\n"
     ]
    }
   ],
   "source": [
    "#install kubeflow pipeline sdk\n",
    "!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if installation was successful\n",
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for pipeline\n",
    "import kfp\n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "import os\n",
    "import subprocess\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create  directory for outputs.\n",
    "output_dir = \"/home/jovyan/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/g04-brazillian-commerce/pipeline'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create preprocess pipeline\n",
    "def download_dataset(data_path):\n",
    "    \n",
    "    # Import Libraries\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras==1.2.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'nltk==3.2.5']) \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    #downloading the dataset\n",
    "    url = 'https://raw.githubusercontent.com/HamoyeHQ/g04-brazillian-commerce/master/data/olist_order_reviews_dataset.csv'\n",
    "    \n",
    "    \n",
    "    #reading data from url\n",
    "    review_df = pd.read_csv(url)\n",
    "    #Save the whole_data as a pickle file to be used by the preprocess component.\n",
    "    #review_df.to_pickle(f'{data_path}/data.pkl')\n",
    "    #np.savez_compressed(f'{data_path}/dataset.npz', \n",
    "                       #data = review_df)\n",
    "    with open(f'{data_path}/dataset','wb') as f:\n",
    "        pickle.dump((review_df),f)\n",
    "        \n",
    "    return (print('Dataset Downloaded'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Downloaded\n"
     ]
    }
   ],
   "source": [
    "download_dataset(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create preprocess pipeline\n",
    "def clean_data(data_path):\n",
    "    \n",
    "    # Import Libraries\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras==1.2.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'nltk==3.2.5']) \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Load and unpack the downloaded dataset\n",
    "    #review_df = pd.read_pickle(f'{data_path}/data.pkl')\n",
    "    with open(f'{data_path}/dataset','rb') as f:\n",
    "        review_df = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    #reading data\n",
    "    #review_df = data\n",
    "    \n",
    "    # Preprocessing the reviews dataset\n",
    "    review_data_title = review_df['review_comment_title']\n",
    "    review_data = review_df.drop(['review_comment_title'],axis=1)\n",
    "\n",
    "    # Dropping NaN values\n",
    "    review_data  = review_data.dropna()\n",
    "    review_data_title = review_data_title.dropna()\n",
    "\n",
    "    # Resetting the reviews index and visualizing the data\n",
    "    review_data = review_data.reset_index(drop=True)\n",
    "    review_data_title = review_data_title.reset_index(drop=True)\n",
    "    \n",
    "    #Save the whole_data as a pickle file to be used by the clean_data component.\n",
    "    #review_data.to_pickle(f'{data_path}/clean_data.pkl')\n",
    "    with open(f'{data_path}/clean_data','wb') as f:\n",
    "        pickle.dump((review_data),f)\n",
    "        \n",
    "    return (print('Dataset Cleaned'), review_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Dataset Cleaned\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, (41753, 6))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create preprocess pipeline\n",
    "def map_review_rating(data_path):\n",
    "    \n",
    "    # Import Libraries\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras==1.2.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'nltk==3.2.5']) \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from sklearn.model_selection import train_test_split\n",
    "     \n",
    "    # Load and unpack the downloaded dataset\n",
    "    with open(f'{data_path}/clean_data','rb') as f:\n",
    "        review_data = pickle.load(f)\n",
    "    \n",
    "    # Mapping the ratings\n",
    "    review_data['Sentiment_rating'] = np.where(review_data.review_score > 3,1,0)\n",
    "    \n",
    "    # Removing neutral reviews \n",
    "    review_data = review_data[review_data.review_score != 3]\n",
    "    \n",
    "    #Save the whole_data as a pickle file to be used by the clean_data component.\n",
    "    with open(f'{data_path}/rating_mapped','wb') as f:\n",
    "        pickle.dump((review_data),f)\n",
    "        \n",
    "    return (print('Rating mapped!'), review_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Rating mapped!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, (38088, 7))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_review_rating(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create preprocess pipeline\n",
    "def text_preprocessing(data_path):\n",
    "    \n",
    "    # Import Libraries\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras==1.2.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'nltk==3.2.5']) \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Load and unpack the downloaded dataset\n",
    "    with open(f'{data_path}/rating_mapped','rb') as f:\n",
    "        review_data = pickle.load(f)\n",
    "    \n",
    "    #making the target categorical\n",
    "    labels = np.array(review_data['Sentiment_rating'])\n",
    "    y = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == 0:\n",
    "            y.append(0)\n",
    "        if labels[i] == 1:\n",
    "            y.append(1)\n",
    "    y = np.array(y)\n",
    "    labels = tf.keras.utils.to_categorical(y, 3, dtype=\"float32\")\n",
    "    del y\n",
    "    \n",
    "    # Getting rid of stopwords, tokenizing and making the text lowercase\n",
    "    comments = []\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    \n",
    "    for words in review_data['review_comment_message']:\n",
    "        only_letters = re.sub(\"[^a-zA-Z]\", \" \",words)\n",
    "        tokens = nltk.word_tokenize(only_letters) #tokenize the sentences\n",
    "        lower_case = [l.lower() for l in tokens] #convert all letters to lower case\n",
    "        filtered_result = list(filter(lambda l: l not in stop_words, lower_case)) #Remove stopwords from the comments\n",
    "        comments.append(' '.join(filtered_result))\n",
    "    \n",
    "    # making the text an array\n",
    "    data = np.array(comments)\n",
    "\n",
    "    max_words = 5000\n",
    "    max_len = 200\n",
    "\n",
    "    # Encoding and padding the texts\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    sequences = tokenizer.texts_to_sequences(data)\n",
    "    encoded_data = pad_sequences(sequences, maxlen=max_len)\n",
    "    \n",
    "    \n",
    "    #Save the whole_data as a pickle file to be used by the preprocess component.\n",
    "    with open(f'{data_path}/model_data','wb') as f:\n",
    "        pickle.dump((encoded_data, labels),f)\n",
    "        \n",
    "    \n",
    "    return (print('Text preprocessing done!'), encoded_data.shape, labels.shape, review_data.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Text preprocessing done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, (38088, 200), (38088, 3), (38088, 7))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessing(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train pipeline\n",
    "def train(data_path):\n",
    "    \n",
    "    # Import Libraries\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras==1.2.2'])\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras import regularizers\n",
    "    from tensorflow.keras import backend as K\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Load and unpack the clean_data\n",
    "    with open(f'{data_path}/model_data','rb') as f:\n",
    "        encoded_data, labels = pickle.load(f)\n",
    "        \n",
    "    \n",
    "    max_words = 5000\n",
    "    max_len = 200   \n",
    "    # Separate the independent data (X) from the dependent data(y).\n",
    "    #encoded_data, labels = clean_data\n",
    "    \n",
    "    #Splitting the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(encoded_data,labels, random_state=0)\n",
    "    \n",
    "    # Building and training a bidirectional LSTM model\n",
    "    classifier = Sequential()\n",
    "    classifier.add(layers.Embedding(max_words, 40, input_length=max_len))\n",
    "    classifier.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\n",
    "    classifier.add(layers.Dense(3,activation='softmax'))\n",
    "    classifier.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('/home/jovyan/', monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "    classifier.fit(X_train,y_train, epochs=25,validation_data=(X_test, y_test),callbacks=[checkpoint])\n",
    "    \n",
    "    classifier.summary()\n",
    "    \n",
    "    #checking the model loss\n",
    "    test_loss, test_acc = classifier.evaluate(X_test, y_test, verbose=2)\n",
    "    print('Model accuracy: ',test_acc) \n",
    "    \n",
    "    #Save the model to the designated \n",
    "    classifier.save(f'{data_path}/sentiment_model.h5')\n",
    "    \n",
    "    #Save the test_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/test_data', 'wb') as f:\n",
    "        pickle.dump((X_test,  y_test), f)\n",
    "    \n",
    "    return (print('Training Done!'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/g04-brazillian-commerce/pipeline'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "Train on 28566 samples, validate on 9522 samples\n",
      "Epoch 1/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.8774\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.92449, saving model to /home/jovyan/best_model.hdf5\n",
      "28566/28566 [==============================] - 105s 4ms/sample - loss: 0.2936 - accuracy: 0.8774 - val_loss: 0.2025 - val_accuracy: 0.9245\n",
      "Epoch 2/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.2061 - accuracy: 0.9204\n",
      "Epoch 00002: val_accuracy improved from 0.92449 to 0.92733, saving model to /home/jovyan/best_model.hdf5\n",
      "28566/28566 [==============================] - 99s 3ms/sample - loss: 0.2062 - accuracy: 0.9204 - val_loss: 0.1933 - val_accuracy: 0.9273\n",
      "Epoch 3/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9261\n",
      "Epoch 00003: val_accuracy improved from 0.92733 to 0.93037, saving model to /home/jovyan/best_model.hdf5\n",
      "28566/28566 [==============================] - 98s 3ms/sample - loss: 0.1940 - accuracy: 0.9262 - val_loss: 0.1908 - val_accuracy: 0.9304\n",
      "Epoch 4/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1862 - accuracy: 0.9302\n",
      "Epoch 00004: val_accuracy did not improve from 0.93037\n",
      "28566/28566 [==============================] - 96s 3ms/sample - loss: 0.1863 - accuracy: 0.9302 - val_loss: 0.1893 - val_accuracy: 0.9297\n",
      "Epoch 5/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1817 - accuracy: 0.9316\n",
      "Epoch 00005: val_accuracy improved from 0.93037 to 0.93142, saving model to /home/jovyan/best_model.hdf5\n",
      "28566/28566 [==============================] - 97s 3ms/sample - loss: 0.1817 - accuracy: 0.9316 - val_loss: 0.1872 - val_accuracy: 0.9314\n",
      "Epoch 6/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1765 - accuracy: 0.9346\n",
      "Epoch 00006: val_accuracy improved from 0.93142 to 0.93342, saving model to /home/jovyan/best_model.hdf5\n",
      "28566/28566 [==============================] - 96s 3ms/sample - loss: 0.1764 - accuracy: 0.9346 - val_loss: 0.1886 - val_accuracy: 0.9334\n",
      "Epoch 7/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1742 - accuracy: 0.9361\n",
      "Epoch 00007: val_accuracy did not improve from 0.93342\n",
      "28566/28566 [==============================] - 96s 3ms/sample - loss: 0.1743 - accuracy: 0.9361 - val_loss: 0.1874 - val_accuracy: 0.9326\n",
      "Epoch 8/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1707 - accuracy: 0.9381\n",
      "Epoch 00008: val_accuracy improved from 0.93342 to 0.93415, saving model to /home/jovyan/best_model.hdf5\n",
      "28566/28566 [==============================] - 97s 3ms/sample - loss: 0.1707 - accuracy: 0.9381 - val_loss: 0.1837 - val_accuracy: 0.9342\n",
      "Epoch 9/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1675 - accuracy: 0.9395\n",
      "Epoch 00009: val_accuracy did not improve from 0.93415\n",
      "28566/28566 [==============================] - 97s 3ms/sample - loss: 0.1675 - accuracy: 0.9395 - val_loss: 0.1860 - val_accuracy: 0.9330\n",
      "Epoch 10/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1648 - accuracy: 0.9407\n",
      "Epoch 00010: val_accuracy improved from 0.93415 to 0.93436, saving model to /home/jovyan/best_model.hdf5\n",
      "28566/28566 [==============================] - 97s 3ms/sample - loss: 0.1649 - accuracy: 0.9407 - val_loss: 0.1846 - val_accuracy: 0.9344\n",
      "Epoch 11/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9425\n",
      "Epoch 00011: val_accuracy improved from 0.93436 to 0.93489, saving model to /home/jovyan/best_model.hdf5\n",
      "28566/28566 [==============================] - 97s 3ms/sample - loss: 0.1614 - accuracy: 0.9426 - val_loss: 0.1844 - val_accuracy: 0.9349\n",
      "Epoch 12/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1585 - accuracy: 0.9422\n",
      "Epoch 00012: val_accuracy improved from 0.93489 to 0.93594, saving model to /home/jovyan/best_model.hdf5\n",
      "28566/28566 [==============================] - 98s 3ms/sample - loss: 0.1585 - accuracy: 0.9422 - val_loss: 0.1862 - val_accuracy: 0.9359\n",
      "Epoch 13/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1590 - accuracy: 0.9435\n",
      "Epoch 00013: val_accuracy did not improve from 0.93594\n",
      "28566/28566 [==============================] - 96s 3ms/sample - loss: 0.1590 - accuracy: 0.9434 - val_loss: 0.1847 - val_accuracy: 0.9328\n",
      "Epoch 14/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1555 - accuracy: 0.9457\n",
      "Epoch 00014: val_accuracy did not improve from 0.93594\n",
      "28566/28566 [==============================] - 96s 3ms/sample - loss: 0.1555 - accuracy: 0.9457 - val_loss: 0.1946 - val_accuracy: 0.9332\n",
      "Epoch 15/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1535 - accuracy: 0.9459\n",
      "Epoch 00015: val_accuracy did not improve from 0.93594\n",
      "28566/28566 [==============================] - 98s 3ms/sample - loss: 0.1536 - accuracy: 0.9459 - val_loss: 0.1867 - val_accuracy: 0.9338\n",
      "Epoch 16/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1510 - accuracy: 0.9461\n",
      "Epoch 00016: val_accuracy did not improve from 0.93594\n",
      "28566/28566 [==============================] - 96s 3ms/sample - loss: 0.1510 - accuracy: 0.9461 - val_loss: 0.1873 - val_accuracy: 0.9317\n",
      "Epoch 17/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1513 - accuracy: 0.9464\n",
      "Epoch 00017: val_accuracy did not improve from 0.93594\n",
      "28566/28566 [==============================] - 95s 3ms/sample - loss: 0.1516 - accuracy: 0.9463 - val_loss: 0.1888 - val_accuracy: 0.9327\n",
      "Epoch 18/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1497 - accuracy: 0.9479\n",
      "Epoch 00018: val_accuracy did not improve from 0.93594\n",
      "28566/28566 [==============================] - 96s 3ms/sample - loss: 0.1497 - accuracy: 0.9479 - val_loss: 0.1901 - val_accuracy: 0.9342\n",
      "Epoch 19/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1497 - accuracy: 0.9477\n",
      "Epoch 00019: val_accuracy did not improve from 0.93594\n",
      "28566/28566 [==============================] - 97s 3ms/sample - loss: 0.1497 - accuracy: 0.9477 - val_loss: 0.1902 - val_accuracy: 0.9317\n",
      "Epoch 20/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1470 - accuracy: 0.9502\n",
      "Epoch 00020: val_accuracy did not improve from 0.93594\n",
      "28566/28566 [==============================] - 96s 3ms/sample - loss: 0.1469 - accuracy: 0.9502 - val_loss: 0.1888 - val_accuracy: 0.9324\n",
      "Epoch 21/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1471 - accuracy: 0.9491\n",
      "Epoch 00021: val_accuracy did not improve from 0.93594\n",
      "28566/28566 [==============================] - 96s 3ms/sample - loss: 0.1470 - accuracy: 0.9492 - val_loss: 0.1952 - val_accuracy: 0.9338\n",
      "Epoch 22/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1454 - accuracy: 0.9499\n",
      "Epoch 00022: val_accuracy did not improve from 0.93594\n",
      "28566/28566 [==============================] - 97s 3ms/sample - loss: 0.1456 - accuracy: 0.9498 - val_loss: 0.1909 - val_accuracy: 0.9294\n",
      "Epoch 23/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1447 - accuracy: 0.9506\n",
      "Epoch 00023: val_accuracy did not improve from 0.93594\n",
      "28566/28566 [==============================] - 95s 3ms/sample - loss: 0.1447 - accuracy: 0.9506 - val_loss: 0.1927 - val_accuracy: 0.9328\n",
      "Epoch 24/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1449 - accuracy: 0.9508\n",
      "Epoch 00024: val_accuracy did not improve from 0.93594\n",
      "28566/28566 [==============================] - 95s 3ms/sample - loss: 0.1448 - accuracy: 0.9509 - val_loss: 0.1939 - val_accuracy: 0.9306\n",
      "Epoch 25/25\n",
      "28544/28566 [============================>.] - ETA: 0s - loss: 0.1430 - accuracy: 0.9502\n",
      "Epoch 00025: val_accuracy did not improve from 0.93594\n",
      "28566/28566 [==============================] - 96s 3ms/sample - loss: 0.1431 - accuracy: 0.9502 - val_loss: 0.1943 - val_accuracy: 0.9325\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 40)           200000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 40)                9760      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 123       \n",
      "=================================================================\n",
      "Total params: 209,883\n",
      "Trainable params: 209,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "9522/1 - 7s - loss: 0.1331 - accuracy: 0.9325\n",
      "Model accuracy:  0.93247217\n",
      "Training Done!\n"
     ]
    }
   ],
   "source": [
    "classifier = train(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_path):\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras==1.2.2'])\n",
    "    import numpy as np\n",
    "    import tensorflow \n",
    "    import keras\n",
    "    \n",
    "    \n",
    "    # Load the saved Keras model\n",
    "    classifier = tensorflow.keras.models.load_model(f'{data_path}/sentiment_model.h5')\n",
    "\n",
    "    # Load and unpack the test_data\n",
    "    with open(f'{data_path}/test_data','rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "        \n",
    "    # Separate the X_test from y_test.\n",
    "    X_test,  y_test = test_data\n",
    "\n",
    "    # make predictions.\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # create a threshold\n",
    "    y_pred=(y_pred>0.5)\n",
    "    \n",
    "    with open(f'{data_path}/result.txt', 'w') as result:\n",
    "        result.write(\" Prediction: {}, Actual: {} \".format(y_pred,y_test.astype(\"int64\")))\n",
    "    \n",
    "    print('Prediction has be saved successfully!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction has be saved successfully!\n"
     ]
    }
   ],
   "source": [
    "predict(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocess, train and predict lightweight components.\n",
    "download_op = comp.func_to_container_op(download_dataset , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "clean_op = comp.func_to_container_op(clean_data , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "map_rating_op = comp.func_to_container_op(map_review_rating , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "text_preprocess_op = comp.func_to_container_op(text_preprocessing , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "train_op = comp.func_to_container_op(train , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "predict_op = comp.func_to_container_op(predict , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build kubeflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a client to enable communication with the Pipelines API server.\n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kubernetes as k8s\n",
    "from kfp import compiler, dsl\n",
    "from kubernetes.client import V1VolumeMount\n",
    "\n",
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Sentiment Analysis Pipeline',\n",
    "   description='An ML pipeline that performs sentiment analysis model training and prediction on customer reviews .'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def sentiment_container_pipeline(\n",
    "    data_path: str,\n",
    "    model_file: str\n",
    "):\n",
    "    #mount_folder = \"/tmp\"\n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"create_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    #op = dsl.PipelineVolume(pvc = vop, volume = )\n",
    "    \n",
    "    #volume = dsl.PipelineVolume(volume=k8s.client.V1Volume(\n",
    "            #name=f\"test-storage\",\n",
    "            #empty_dir=k8s.client.V1EmptyDirVolumeSource()))\n",
    "    \n",
    "    #op2.container.add_volume_mount(volume_mount=V1VolumeMount(mount_path=mount_folder,\n",
    "                                                              #name=volume.name))\n",
    "    #op2.after(op)\n",
    "    \n",
    "    # Create sentiment analysis preprocess component.\n",
    "    sentiment_download_container = download_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: vop.volume})\n",
    "    \n",
    "     # Create sentiment analysis preprocess component.\n",
    "    sentiment_clean_container = clean_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: sentiment_download_container.pvolume})\n",
    "    \n",
    "     # Create sentiment analysis preprocess component.\n",
    "    sentiment_rating_container = map_rating_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: sentiment_clean_container.pvolume})\n",
    "    \n",
    "     # Create sentiment analysis preprocess component.\n",
    "    sentiment_textpreprocess_container = text_preprocess_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: sentiment_rating_container.pvolume})\n",
    "    \n",
    "    # Create sentiment analysis training component.\n",
    "    sentiment_training_container = train_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: sentiment_textpreprocess_container.pvolume})\n",
    "                                    #{data_path: sentiment_textpreprocess_container.pvolume})\n",
    "     #sentiment_textpreprocess_container.container.add_volume_mount(volume_mount=V1VolumeMount(mount_path=mount_folder,\n",
    "                                                              #name=volume.name))\n",
    "    #sentiment_textpreprocess_container.after(train_op)\n",
    "    #sentiment_training_container = train_op(data_path) \\\n",
    "                                    #.add_pvolumes({data_path: sentiment_textpreprocess_container.pvolume})\n",
    "\n",
    "    # Create sentiment analysis prediction component.\n",
    "    sentiment_predict_container = predict_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: sentiment_training_container.pvolume})\n",
    "    \n",
    "    # Print the result of the prediction\n",
    "    sentiment_result_container = dsl.ContainerOp(\n",
    "        name=\"print_prediction\",\n",
    "        image='library/bash:4.4.23',\n",
    "        pvolumes={data_path: sentiment_predict_container.pvolume},\n",
    "        arguments=['cat', f'{data_path}/result.txt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt'\n",
    "MODEL_PATH='olist_ecommerce_sentiment_analysis.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = sentiment_container_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/e4160ac1-0f66-4e3c-8936-2ef3f11b1c3d\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/014af382-024d-46bc-93f8-b4eaacb19bd9\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'sentiment_analysis_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH,\n",
    "            \"model_file\":MODEL_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
